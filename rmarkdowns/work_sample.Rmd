---
title: "Work Sample"
author: "Clyde Schwab"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(tidyverse)
library(foreign)
library(ggplot2)
library(MASS)
library(Hmisc)
library(reshape2)
library(brant)
library(sf)
library(tigris)
library(tmap)
library(sp)
library(spdep)
library(rmapshaper)
library(rgdal)
library(gplots)
library(lmtest)
library(sandwich)
library(sjPlot)
setwd("~/Desktop/Code/Thesis")
```

This document represents a small sample of work I've done recently related to data cleaning and analysis for my BA thesis project on gentrification in Berlin. I've generally reduced a lot of code into a few select samples, and I'm working on having a more complete project website within the month! 

The first section focuses on cleaning data and generating data I use for my thesis, drawn from two demographic surveys and a proprietary datasets from the Berlin Senate's Office of Urban Planning and Development. I detail a small sample of loading and cleaning done to this data, and then demonstrate how I use it to create a novel, conceptually-driven approach to typologizing stages of gentrification according to the invasion-succession model. 

In the second section, I provide a very brief sample of statistical analysis I've done for my thesis. 

If you have any questions feel free to reach out via [email](cschwab1@uchicago.edu), and I look forward to hearing from you! s

## 1: Data Loading and Cleaning

To create my gentrification typology, I used data from the resident register (Einwohnerregister), a survey called Monitoring Social Urban Development (MSS, Monitoring Sozialestadtentwicklung), and data on the average sale price on residential units. This data was largely aggregated to the [LOR Planning Region area](https://www.stadtentwicklung.berlin.de/planen/basisdaten_stadtentwicklung/lor/), but some years needed to be arealy interpolated. For this example, I'll show how I processed and combined data with a single year as an example (for combination I use two years, as I'm basing my typology off change over each two year period). 

The data, while publicly available in reports since the year 2000, is not processed into csv files, and I instead acquired these by reaching out to representatives at the Urban Development office. While briefly cleaned in each year, the names of the variables vary substantially over the years and must thus be standardized. 

### 1.1 MSS Data
Here's the basic process for the MSS report data: 

```{r MSS, message=FALSE, warning=FALSE}
library(tidyverse)
########## MSS 2011
mss_2011 <- read.csv("~/Desktop/Code/Thesis/demographic/2011_MSS_cut.csv")
mss_2011 <- mss_2011[1:447,]
mss_2011 <- mss_2011 %>% 
  dplyr::select("Gebiet",
                "Raumid",
                "Dynamik2",
                "E11",
                "E12",
                "E13",
                "E14",
                "E17",
                "E15",
                "E22",
                "E23",
                "Status1") %>%
  rename(    WA = Dynamik2,
             turk = E11,
             arab = E12,
             easteuro = E13,
             pol = E17,
             yugo = E14,
             eu = E15,
             unemp = Status1,
             gwelf = E22,
             awelf = E23)
mss_2011[,2:12] <- lapply(mss_2011[,2:12], as.numeric)
mss_2011$aus_noneu <- mss_2011$turk + mss_2011$arab + mss_2011$easteuro + mss_2011$pol + mss_2011$yugo
mss_2011$welf <- mss_2011$gwelf + mss_2011$awelf
mss_2011 <- dplyr::select(mss_2011, 
                   "Raumid",
                   "Gebiet",
                   "WA",
                   "aus_noneu",
                   "eu",
                   "welf",
                   "unemp")
```

Datasets from 2001 onwards are arealy interpolated to the planning region level using `aw_interpolate()` from the `areal` package. 

### 1.2 Einwohnerregister Data

Resident register data has similarly heterogeneous variable names (though this isn't shown in the example). Data is available for download  [here](https://daten.berlin.de/tags/kleinr%C3%A4umige-einwohnerzahl). 

```{r eval=FALSE}
# 2011
w_2011 <- read.table("~/Desktop/Code/Thesis/EinR/2011/WHNDAUER2010_Matrix.csv", header=TRUE, sep = ";")
aus_2011 <- read.table("~/Desktop/Code/Thesis/EinR/2011/EWR201012A_Matrix.csv", header=TRUE, sep = ";")
ein_2011 <- read.table("~/Desktop/Code/Thesis/EinR/2011/EWR201012E_Matrix.csv", header=TRUE, sep = ";")
einW11 <- merge(ein_2011, aus_2011, by="RAUMID") %>% merge(w_2011, by="RAUMID")
einW11 <- einW11 %>% mutate(E_0U6 = E_U1 + E_1U6) %>%
  dplyr::select("RAUMID", "E_E", 
                 # helpful age ranges
                 "E_18U25", "E_25U55", "E_U1", "E_0U6",
                 # foreigners 
                 "E_A", 
                 # people who have been living in the neighborhood for 5-10 years
                 "DAU10", "DAU5", "PDAU10", "PDAU5")
einW11$PDAU10 <- gsub(",", ".", einW11$PDAU10) %>% as.numeric()
einW11$PDAU5 <- gsub(",", ".", einW11$PDAU5) %>% as.numeric()
rm(ein_2011, aus_2011, w_2011)
```


### 1.3 Sale-price data

Processing average price per area requires a more involved process of interpolation via kriging. While due to licensing issues I can't share the base data I use in this interpolation, the following code shows (1) how I load and clean the sale-price data, (2) how I download shapefiles from the Berlin data portal for interpolation, and (3) the function I use to interpolate the data. 

```{r eval=FALSE}
##### Step 1: Loading GAA data
setClass("num.with.commas")
setAs("character", "num.with.commas", 
      function(from) as.numeric(gsub(",", "", from) ) )
gaa <- read.csv("~/Desktop/Code/Thesis/gaa_etwsab2000_clean.csv", 
                         colClasses=c('num.with.commas')) %>% 
  dplyr::select(1:45) %>% 
  dplyr::select("Block", "m_2000", "m_2001", "m_2002", "m_2003", "m_2004", "m_2005", 
                "m_2006", "m_2007", "m_2008", "m_2009", "m_2010", "m_2011",
                "m_2012", "m_2013", "m_2014", "m_2015", "m_2016", "m_2017", 
                "m_2018", "m_2019", "m_2020", "m_2021") 

gaa <- left_join(gaa, sb, by=c("Block" = "blknr")) %>% 
  st_as_sf() 
gaa <- st_centroid(gaa) %>% filter(!st_is_empty(.))

##### Step 2: Functions required to download spatial data: 
get_X_Y_coordinates <- function(x) {
  sftype <- as.character(sf::st_geometry_type(x, by_geometry = FALSE))
  if(sftype == "POINT") {
    xy <- as.data.frame(sf::st_coordinates(x))
    dplyr::bind_cols(x, xy)
  } else {
    x
  }
}

sf_fisbroker <- function(url) {
  typenames <- basename(url)
  url <- httr::parse_url(url)
  url$query <- list(service = "wfs",
                    version = "2.0.0",
                    request = "GetFeature",
                    srsName = "EPSG:25833",
                    TYPENAMES = typenames)
  request <- httr::build_url(url)
  print(request)
  out <- sf::read_sf(request)
  out <- sf::st_transform(out, 3035)
  out <- get_X_Y_coordinates(out)
  out <- st_as_sf(as.data.frame(out))
  return(out)
}

# Downloading file for statistical blocks
sb <- sf_fisbroker("https://fbinter.stadt-berlin.de/fb/wfs/data/senstadt/s_rbs_bloecke")

##### Step 3: Interpolation 

# creating base raster
load(url("https://userpage.fu-berlin.de/soga/300/30100_data_sets/berlin_district.RData"))
berlin.sf <- st_transform(berlin.sf, 3035)
berlin_template <- raster(extent(berlin.sf), 
                          # 300 is the length of three square Berlin blocks (the average size of a berlin block is 45,000 sqm)
                          resolution = 367, 
                          crs = st_crs(berlin_base)$proj4string)
bbase <- as(berlin_template, "SpatialPixels")

# function to use kriging to interpolate average price over three years
gaa_calc <- function(col1, col2, col3){
  newdf <- gaa[c("Block", col1, col2, col3)] %>% st_drop_geometry()
  newdf$newcol <- rowMeans(newdf[2:4], na.rm = TRUE)
  newdf <- newdf %>% filter(newcol != "NaN")
  newdf <- newdf[c("Block", "newcol")]
  newdf_sf <- left_join(newdf, gaa[-c(2:24)], by=c("Block")) %>% st_as_sf() %>% as_Spatial()
  crs(newdf_sf) <- "+proj=laea +lat_0=52 +lon_0=10 +x_0=4321000 +y_0=3210000 +ellps=GRS80 +units=m +no_defs"
  x <- autoKrige(formula=newcol~1, newdf_sf, new_data=bbase, model = c("Sph", "Exp", "Gau", "Ste", "Nug"))
  y <- x$krige_output
  z <- raster(y)
  out <- mask(x = z, mask = berlin_base)
  return(out)
}

# interpolating over the two previous target years
gaa09k <- gaa_calc("m_2008", "m_2009", "m_2010")
```

An example of interpolation at a few different years is shown below:
```{r echo=FALSE, message=FALSE, warning=FALSE}
load("~/Desktop/Code/Thesis/Data_for_Analysis/gaakrigd.Rdata")
breaks <- c(0, 1.5, 3, 4.5, 6, 7.5, 9) * 1000
map01 <- tm_shape(gaa01k) + tm_raster(title="sale price per square meter", breaks = breaks, palette = "Blues") + tm_layout(title = "Berlin Land Value: 2001")
map02 <- tm_shape(gaa09k) + tm_raster(title="sale price per square meter", breaks = breaks, palette = "Blues") + tm_layout(title = "Berlin Land Value: 2009")
map03 <- tm_shape(gaa19k) + tm_raster(title="sale price per square meter", breaks = breaks, palette = "Blues") + tm_layout(title = "Berlin Land Value: 2019")
tmap_mode("plot")
tmap_arrange(map01, map02, map03, nrow = 1)
```

As we can see, the average price per square meter grows very substantially across the city from 2001-2019. 

After interpolation, the data is aggregated to the LOR planning area level. 

```{r eval=FALSE}
gaa09sf <- gaa09k %>% as('SpatialPolygonsDataFrame') %>% st_as_sf()

##### LOR
lor <- sf_fisbroker("https://fbinter.stadt-berlin.de/fb/wfs/data/senstadt/s_lor_plan")
lor <- lor %>% 
  separate(gml_id, c("s_lor_plan", "gml_id"), sep=11) %>%
  dplyr::select("gml_id", "geometry")
st_crs(lor) <- st_crs(gaa09sf)

gaalorjoin <- function(x){
  out <- st_join(lor, x, join = st_intersects, left = TRUE) %>% 
    group_by(gml_id) %>% 
    summarize(var1.pred = mean(var1.pred, na.rm=TRUE))
  return(out)
}

gaa09lor <- gaalorjoin(gaa09sf) %>% rename(gaa09 = var1.pred) %>% st_drop_geometry()
gaa11lor <- gaalorjoin(gaa11sf) %>% rename(gaa11 = var1.pred) %>% st_drop_geometry()
```

Processing the original data is done for each of the 10 observation periods. 

### 1.4 Protected areas processing (treatment)

While this isn't used in stage two, here I can demonstrate a few ways I used data on which areas in Berlin were affected by the policy I was interested in: 

```{r eval=FALSE}
library(rgdal)
library(lubridate)
library(lazyeval)

eg <- sf_fisbroker("https://fbinter.stadt-berlin.de/fb/wfs/data/senstadt/s_erhaltgeb_em")
eg <- eg
eg <- eg %>% separate(gml_id, c("gml_id", "ID"), sep=17)
eg$ID <- eg$ID %>% as.numeric()

eg$F_IN_KRAFT <- as.Date(eg$F_IN_KRAFT, format = "%d.%m.%Y")

timecalc <- function(df, from_year){
  x <- as_date(from_year)
  df$y <- as.numeric(difftime(x, df$F_IN_KRAFT, unit="weeks"))/52.25
  df$y[df$y <= 0] <- NA
  return(df)
}

eg <- timecalc(eg, "2000-12-31") %>% rename(yrs01 = y)
eg <- timecalc(eg, "2002-12-31") %>% rename(yrs03 = y)
# [...] normally this is done for all years, but I didn't want code to be too messy! 
eg <- timecalc(eg, "2021-01-01") %>% rename(yrs21 = y)

eg <- st_centroid(eg)

eg_lor <- st_join(lor, eg, join=st_intersects) %>% distinct(PLANUNGSRAUM, .keep_all = TRUE)

eg_lor_500 <- st_join(lor, st_as_sf(eg), join = st_is_within_distance, dist = 500) %>% 
  distinct(PLANUNGSRAUM, .keep_all = TRUE)
eg_lor_500 <- eg_lor_500 %>% dplyr::select("gml_id.x", "gml_id.y", "PLANUNGSRAUM", "GEBIETSNAME", "F_IN_KRAFT", "geometry")

eg_lor_500 <- timecalc(eg_lor_500, "2000-12-31") %>% rename(yrs01_500 = y)
eg_lor_500 <- timecalc(eg_lor_500, "2002-12-31") %>% rename(yrs03_500 = y)
# [...]
eg_lor_500 <- timecalc(eg_lor_500, "2021-01-01") %>% rename(yrs21_500 = y)
eg_lor_500 <- eg_lor_500 %>% dplyr::select(1:3, 5:16)

eg_lor_full <- left_join(as.data.frame(eg_lor), eg_lor_500, by=c("gml_id.x"))

# function to create dummy variable for whether an area is treated in each period of observation; this is useful when data is converted to long format
egdummy <- function(df, fromyear){
  x <- as_date(fromyear)
  df$y <- if_else(df$F_IN_KRAFT.x <= x, 1, 0)
  return(df)
}

eg_lor_full <- egdummy(eg_lor_full, "2000-12-31") %>% rename(iseg01 = y)
eg_lor_full <- egdummy(eg_lor_full, "2002-12-31") %>% rename(iseg03 = y)
# [...]
eg_lor_full <- egdummy(eg_lor_full, "2018-12-31") %>% rename(iseg19 = y)

# the process is repeated for areas within 500 meters. 
```

## 2: Gentrification data generation

### 2.1 Data combining 
Before data is generated, all three clean datasources have been saved as `.Rdata` files and are loaded and subsequently combined in each year. Then, years subtracted to find the change in each variable between each year. Once again, only a sample of this process is shown. 

```{r eval=FALSE}
load("Data_for_Analysis/einR.Rdata")
load("Data_for_Analysis/gaadata.Rdata")
load("Data_for_Analysis/mss.Rdata")

mss_2009$Nr. <- as.numeric(mss_2009$Nr.)
gaa09lor$gml_id <- as.numeric(gaa09lor$gml_id)
b09 <- left_join(einW09, mss_2009, by=c("RAUMID" = "Nr.")) %>% 
  left_join(., gaa09lor, by=c("RAUMID" = "gml_id"))

mss_2011$Raumid <- as.numeric(mss_2011$Raumid)
gaa11lor$gml_id <- as.numeric(gaa11lor$gml_id)
b11 <- left_join(einW11, mss_2011, by=c("RAUMID" = "Raumid")) %>% 
  left_join(., gaa11lor, by=c("RAUMID" = "gml_id"))

b11c <- left_join(b11, b09, by=c("RAUMID")) %>% dplyr::select(1:6, 13, 15, 14, 16:18, 8:11, 19:23, 30, 32, 31, 33:35, 25:28)
b11c[2:16] <- b11c[2:16] - b11c[17:31]
b11c <- b11c[1:16]
names(b11c) <- gsub(x = names(b11c), pattern = ".x", replacement = "")
b11c <- b11c %>% rename(gaa = gaa11)
```

The object `b11c` (standing for Berlin 2011 change) is an example of the objects used as the basis for the typologization process. 

### 2.2 typologification

Before showing the function I created to classify each each, I want to explain a bit of the idea behind it. I've based the stages I use off a thorough literature review, with particular attention to Dangschat (1988) and Blasius et al (2015). This model , roughly put, represents the portrait of gentrification that an average contemporary city-dweller imagines upon hearing the term and thinking of examples like Wicker Park (Chicago), or SoHo New York. The process starts in a lower income neighborhood when a few artists and students begin moving in because of cheap rent and available apartments, a permissive environment, and the perception of some more authentic lifestyle. As the area becomes trendier (nightlife, cafes, bars etc), it becomes more attractive for like-minded young urbanites, and the rent increases, thus displacing the original residents and making room for the more risk-averse, older and wealthier urbanites who are nonetheless attracted to the chic neighborhood. Finally, only these wealthy older newcomers can afford it, and the original bohemians have left for some cheaper "undiscovered" neighborhood elsewhere. There are a few underlying assumptions to this model when implemented, most significantly that each area experiencing gentrification follows the same process detailed above, and that gentrification in an area can only increase or stay the same. These aren't necessarily great assumptions, but they'll have to do if this model is to be implemented. 

In terms of actually implementing it, I created a function to classify each area based on the changes experienced by each area in each two-year difference. Basically, the first year (2001) is fed in, where the only significant data is the average sale price per area, used to classify an area as `0`, meaning not susceptible to gentrification based on being above the 70th percentile in sale price per square meter, or `1`, susceptible. The classification for each area is joined on to the data for 2003, which is then fed back into the function. Areas classified in 2001 as `1` are checked to see if they're in stage `2`, the first pioneer stage; if they pass they're assigned `2`, but if not, they remain `1`, and areas classified in 2001 as `0` are checked again based on the original 70th percentile filter. This is repeated for 2005, with the areas classified as `2` in 2003 now able to be checked by the third filter, and so on; this means it is only possible for an area to be classified as `5`, or fully gentrified, in 2009, but from there other areas still classified as `1-4` are rechecked each time, and updated if they pass. 

I have to account for a good bit of variability in the available variables in each year; a table of the variables available per year is shown below, which is be accounted for in the final function, which should be evident (though probably a bit hard to follow)

```{r echo=FALSE}
load("~/Desktop/Code/Thesis/Data_for_Analysis/datacomb.Rdata")
namesdf %>% kable()
```

Without further ado, here's the function that is the basis of my classification system: 
```{r eval=FALSE}
gentriclass <- function(x, y){
   for(i in 1:nrow(x)){
    # Check: if input dataframe includes information on time in neighborhood, process here 
    if("PDAU5" %in% colnames(x)){
      if (isTRUE(x$gcode[i] == 4)){
        # Check 5: classifies for stage IV of gentrification, final gentrifier stage
        # current indicators:
        ### increase in land value (above 60th percentile)
        ### increase in children 0-6 yrs old
        ### decrease in foreigners from outside EU
        ### decrease in welfare
        ### decrease in unemployment
        ### decrease in 18-25s
        ### decrease in people living in neighborhood for 5 & 10 yrs
        if (((x$gaa[i] > quantile(x$gaa, .6)) + (x$E_18U25[i] < 0) + (x$DAU5[i] < 0) + (x$DAU10[i] < 0)) >= 3){
          x$gcode[i] <- 5
          x$gcode5yr[i] <- x$year[i]
        }
      } else if (isTRUE(x$gcode[i] == 3)){
        # Check 4: classifies for stage III of gentrification, early gentrifier stage
        # current indicators:
        ### increase in 25-55
        ### increase in eu
        ### increase in 0-1 yr olds
        ### decrease in welf
        ### decrease in % in neighborhood for more than 5 & 10 yrs
        if (isTRUE(((x$E_25U55[i] > 0) + (x$E_0U6[i] > 0) + (x$PDAU5[i] < 0) + (x$PDAU10[i] < 0)) >= 3)){
          x$gcode[i] <- 4
          x$gcode4yr[i] <- x$year[i]
        }
      } else if (isTRUE(x$gcode[i] == 2)){
        # Check 3: classifies for stage II of gentrification, late pioneer stage/first displacement stage
        # current indicators: 
        ### increase in land value
        ### decrease in those on welfare
        ### decrease in people in neighborhood for 5 yrs, 10 yrs
        if (isTRUE(((x$gaa[i] > 0) + (x$welf[i] < 0) + (x$DAU5[i] < 0) + (x$DAU10[i] < 0)) >= 3)){
          x$gcode[i] <- 3
          x$gcode3yr[i] <- x$year[i]
        }
      } else if (isTRUE(x$gcode[i] == 1)){
        # Check 2: classifies for stage I of gentrification, early pioneer stage
        # current indicators: 
        ### increase in 18-25s (E_18U25)
        ### increase in people from EU
        ### decrease in % people in the neighborhood for > 10 years
          if (isTRUE(x$E_18U25[i] > 0 &&
                     x$eu[i] > 0 &&
                     x$PDAU10[i] < 0)){
            x$gcode[i] <- 2
            x$gcode2yr[i] <- x$year[i]
          }
      } else if (isTRUE(x$gcode[i] == 0)){
        # Check 1: classifies for pre-gentrification areas
        # current indicators:
        ### above 75 percentile of land value: already too expensive to be gentrified
          if (isTRUE(x$gaa[i] <= quantile(x$gaa, .6))){
            x$gcode[i] <- 1
            x$gcode1yr[i] <- x$year[i]
            }
        } 
      } else {
  ########################################################################################################################################################################################################
        # Check: dataframe does not have PDAU variables
        if (isTRUE(x$gcode[i] == 4)){
          # Check 5: classifies for stage IV of gentrification, final gentrifier stage
          # current indicators:
          ### becomes highly valued land in city; above 60th percentile
          ### decrease in welfare and unemployment
          ### increase in children
          ### decrease in 18-25
          ### increase in people from EU
          ### decrease in foreigners not from EU
          if (((x$E_18U25[i] < 0) + (x$welf[i] < 0) + (x$gaa[i] >= quantile(x$gaa, .6))) >= 2){
            x$gcode[i] <- 5
            x$gcode5yr[i] <- x$year[i]
          }
        } else if (isTRUE(x$gcode[i] == 3)){
          # Check 4: classifies for stage III of gentrification, early gentrifier stage
          # current indicators:
          ### increase in 25-55
          ### decrease in welf
          ### increase in eu
          ### increase in < 1 yr olds
          ### increase in 1-6 yr olds
          if (((x$E_25U55[i] > 0) + (x$E_0U6[i] > 0) + (x$eu[i] > 0)) >= 2){
            x$gcode[i] <- 4
            x$gcode4yr[i] <- x$year[i]
          }
        } else if (isTRUE(x$gcode[i] == 2)){
          # Check 3: classifies for stage II of gentrification, late pioneer stage/first displacement stage
          # current indicators: 
          ### increase in land value
          ### decrease in those on welfare
          if (isTRUE(((x$gaa[i] > 0) + (x$welf[i] < 0)) == 2)){
            x$gcode[i] <- 3
            x$gcode3yr[i] <- x$year[i]
          }
        } else if (isTRUE(x$gcode[i] == 1)){
          # Check 2: classifies for stage I of gentrification, early pioneer stage
          # current indicators: 
          ### increase in 18-25s (E_18U25)
          ### increase in people from EU
          if("eu" %in% colnames(x)){
            if ( isTRUE ((x$E_18U25[i] > mean(x$E_18U25)) &&
                         (x$eu[i] > 0)) ){
              x$gcode[i] <- 2
              x$gcode2yr[i] <- x$year[i]
            }
          } else {
            if ( isTRUE (x$E_18U25[i] > mean(x$E_18U25))){
              x$gcode[i] <- 2
              x$gcode2yr[i] <- x$year[i]
            }
          }
        } else if (isTRUE(x$gcode[i] == 0)){
          # Check 1: classifies for pre-gentrification areas
          # current indicators:
          ### above 75 percentile of land value: already too expensive to be gentrified
          if (isTRUE(x$gaa[i] <= quantile(x$gaa, .6))){
            x$gcode[i] <- 1
            x$gcode1yr[i] <- x$year[i]
          }
        } 
      }
    }
  return(x)
}

# example of use: 
b03g <- gentriclass(b01) %>% dplyr::select(c(1:6, 14))  
b03g$gen.01 <- b03g$gcode
b03g <- b03g %>% merge(., b03c, by="RAUMID")
  
b05g <- gentriclass(b03g) %>% dplyr::select(1:8)
b05g$gen.03 <- b05g$gcode
b05g <- b05g %>% merge(., b05c, by="RAUMID")
# [...]
b19g <- gentriclass(b17g) %>% dplyr::select(1:15) 
b19g$gen.17 <- b19g$gcode
b19g <- b19g %>% merge(., b19c, by="RAUMID") 
  
bfinal <- gentriclass(b19g) %>% dplyr::select(1:16)
bfinal$gen.19 <- bfinal$gcode
```

I also measure the amount of time each area spent in each year of gentrification (though I'm still figuring out exactly how to use these numbers for a survival analysis)

```{r eval=FALSE}
s1 <- bfinal %>% filter(!is.na(bfinal$gcode2yr))
s1$tg1 <- time_length(difftime(s1$gcode2yr, s1$gcode1yr), "years")
s1 <- s1[c(1, 18)]
# [...]
s5 <- bfinal %>% filter(!is.na(bfinal$gcode5yr))
s5$tg5 <- time_length(difftime(as.Date("2020-1-1"), s5$gcode5yr), "years")
s5 <- s5[c(1, 18)]
  
bfinal <- bfinal %>% left_join(., s1, by=c("RAUMID")) %>%
  left_join(., s2, by=c("RAUMID")) %>% 
  left_join(., s3, by=c("RAUMID")) %>% 
  left_join(., s4, by=c("RAUMID")) %>%
  left_join(., s5, by=c("RAUMID"))
bfinal[8:12] <- round(bfinal[8:12], digits = 1)
```

Finally, before analysis, all the data (including a few selected demographic variables) is converted into long process according to the example given here: 

```{r eval=FALSE}
load("~/Desktop/Code/Thesis/Data_for_Analysis/bfinal.Rdata")
load("~/Desktop/Code/Thesis/Data_for_Analysis/eg.rda")
load("~/Desktop/Code/Thesis/Data_for_Analysis/pdau.Rdata")

##### some minor formatting before analysis
eg_lor_full$ID <- as.numeric(eg_lor_full$ID)
df <- left_join(bfinal, eg_lor_full, by=c("RAUMID" = "ID"))
df$F_IN_KRAFT.x <- round_date(df$F_IN_KRAFT.x, unit = "years") %>% lubridate::year() %>% as.numeric()
df$F_IN_KRAFT.y <- round_date(df$F_IN_KRAFT.y, unit = "years") %>% lubridate::year() %>% as.numeric()
df <- df[-c(2:6)]
df[is.na(df)] <- 0

##### converting data into long format
# status and protected status 
dfg <- df[c(1, 3:12, 19:20, 32:34)]
dfL_gstatus <- reshape(dfg,
                       direction = "long",
                       varying = c("gen.01", "gen.03", "gen.05", "gen.07", "gen.09", "gen.11",
                                   "gen.13", "gen.15", "gen.17", "gen.19"),
                       v.names = "gstatus",
                       idvar = "RAUMID",
                       timevar = "Year",
                       times = c("2001", "2003", "2005", "2007", "2009", "2011", "2013", "2015", "2017", "2019")) %>%
  tibble::rowid_to_column(., "ID")

dfttime <- df %>% dplyr::select(c(1, 21:30))
dfL_tt <- reshape(dfttime, 
                  direction = "long",
                  varying = list(names(dfttime)[c(2:11)]),
                  v.names = "treattime", 
                  idvar = "RAUMID",
                  timevar = "Year",
                  times = c("2001", "2003", "2005", "2007", "2009", "2011", "2013", "2015", "2017", "2019")) %>%
  tibble::rowid_to_column(., "ID") %>% dplyr::select(c(1, 4))
```

### 2.3 Results of data generation
```{r echo=FALSE}
##### Step 2: Functions required to download spatial data: 
get_X_Y_coordinates <- function(x) {
  sftype <- as.character(sf::st_geometry_type(x, by_geometry = FALSE))
  if(sftype == "POINT") {
    xy <- as.data.frame(sf::st_coordinates(x))
    dplyr::bind_cols(x, xy)
  } else {
    x
  }
}

sf_fisbroker <- function(url) {
  typenames <- basename(url)
  url <- httr::parse_url(url)
  url$query <- list(service = "wfs",
                    version = "2.0.0",
                    request = "GetFeature",
                    srsName = "EPSG:25833",
                    TYPENAMES = typenames)
  request <- httr::build_url(url)
  print(request)
  out <- sf::read_sf(request)
  out <- sf::st_transform(out, 3035)
  out <- get_X_Y_coordinates(out)
  out <- st_as_sf(as.data.frame(out))
  return(out)
}
```

Before moving on, I can help show off the data I generated with some really basic maps: 
```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(sf)
library(tmap)
library(rgdal)
library(gifski)

load("~/Desktop/Code/Thesis/Data_for_Analysis/bfinal.Rdata")
load("~/Desktop/Code/Thesis/Data_for_Analysis/eg.rda")
load("~/Desktop/Code/Thesis/Data_for_Analysis/pdau.Rdata")
load("~/Desktop/Code/Thesis/Data_for_Analysis/blong.Rdata")

lor1 <- sf_fisbroker("https://fbinter.stadt-berlin.de/fb/wfs/data/senstadt/s_lor_plan")
lor1 <- lor1 %>% 
  separate(gml_id, c("s_lor_plan", "gml_id"), sep=11) %>% 
  st_drop_geometry() %>% 
  as.data.frame()
lor <- st_as_sf(readOGR("~/Desktop/Code/Thesis/shapefiles/lor.shp")) %>% rename(RAUMID = gml_id)
lor <- left_join(as.data.frame(lor), lor1[c(2:6)], by=c("RAUMID" = "gml_id")) %>% st_as_sf()
lor$RAUMID <- as.numeric(lor$RAUMID)

# joining objects to shapefile
b_sf <- left_join(bfinal, lor, by=c("RAUMID")) %>% st_as_sf()
bfinal[, 8:16][is.na(bfinal[, 8:16])] <- 0
bwhole <- st_cast(lor, to="POLYGON")

bL_sf <- left_join(dfL_full, lor, by=c("RAUMID")) %>% st_as_sf()
bL_sf$gstatus <- as.factor(bL_sf$gstatus)

# making basic tmap facet
breaks = c(0, 1, 2, 3, 4, 5)
general_facetbyyear <- tm_shape(bL_sf) + 
  tm_polygons(col = "gstatus", breaks = breaks, palette = "Oranges") + 
  tm_facets(by=c("Year"), nrow = 3) + 
  tm_layout(title = "Gentrification Status Bi-Yearly: 2003-2019",
            legend.position = c("right", "bottom"), 
            title.size = 5,
            frame = F)

# facets aggregated to districts (Bezirk) 
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

bezirks_byyear <- bL_sf %>% 
  group_by(BEZIRKSNAME, Year) %>% 
  summarise(modestatus = Mode(gstatus))
bezirksfacet <- tm_shape(bezirks_byyear) + 
  tm_polygons(col = "modestatus", breaks = breaks, palette = "Oranges", title = "Gentrification Status Bi-Yearly: 2003-2019, by district") + 
  tm_facets(by=c("Year"), nrow = 3, scale.factor = 2)
bezirksfacet
```

Here's gif of change on the planning region level as a gif! 

```{r message=FALSE, echo=FALSE, results='hide'}
tmap_mode("plot")
urb_anim <- tm_shape(bL_sf) + 
  tm_polygons(col = "gstatus", breaks = breaks, palette = "Oranges") + 
  tm_facets(along=c("Year"), nrow = 3) + 
  tm_layout(title = "Gentrification Status Bi-Yearly: 2003-2019",
            legend.position = c("right", "bottom"),
            frame = F)
tmap_animation(urb_anim, filename = "urb_anim.gif", delay = 50)
```

```{r animation.hook='gifski', results='hide'}
urb_anim
```

There's a ton of possibilities for cool visualizations, and I'm working on creating a shiny app that can focus in on specific districts to create a facet map/animated gif, as well as illustrating the relationship between treatment and gentrification (including spillover). 

But, without further ado, the next section shows a few methods I used for analysis. 

## 3: Analysis

### 3.1 Ordered Logit 

I started by doing an ordinal logistic regression to estimate the effects of treatment on liklihood to be in any one stage of gentrification. This isn't necessarily going to tell us much, given that areas experiencing gentrification are probably going to be more likely to be selected anyway. The basic output is shown below: 

```{r}
load("~/Desktop/Code/Thesis/Data_for_Analysis/datacomb.Rdata")
load("~/Desktop/Code/Thesis/Data_for_Analysis/gtable.Rdata")
load("~/Desktop/Code/Thesis/Data_for_Analysis/blong.Rdata")

dfL_full$gstatus <- as.factor(dfL_full$gstatus)
dfL_full <- dfL_full %>% filter(F_IN_KRAFT.x < 2020)

m_simple <- MASS::polr(gstatus ~ treattime, data = dfL_full, Hess=TRUE)
summary(m_simple)
brant(m_simple)
(ctables <- coef(summary(m_simple)))
p <- pnorm(abs(ctables[, "t value"]), lower.tail = FALSE) * 2
ctables <- cbind(ctables, "p value" = p)
as.data.frame(ctables)
(ci <- confint(m_simple))
confint.default(m_simple)
exp(coef(m_simple))
exp(cbind(OR = coef(m_simple), ci))
```

While the regression coefficients show that there isn't a substantial effect of being in any one stage and being treated, some more interesting results are given when we look at areas that aren't themselves treated, but are within 500 meters of a treated area

```{r}
df2 <- dfL_full %>% filter(dfL_full$is_eg != 1)
m_500 <- polr(gstatus ~ treattime500, data = df2, Hess=TRUE)
equatiomatic::extract_eq(m_500)
summary(m_500)
brant(m_500)
(ctables <- coef(summary(m_500)))
p <- pnorm(abs(ctables[, "t value"]), lower.tail = FALSE) * 2
ctables <- cbind(ctables, "p value" = p)
as.data.frame(ctables)
(ci <- confint(m_500))
confint.default(m_500)
exp(coef(m_500))
exp(cbind(OR = coef(m_500), ci))
``` 

The coefficient $\beta$ of treatment time is 0.15 with a significance level of < 0.01 and a confidence interval of (0.124, 0.187), indicating that proximity to a protected area has a stronger association between increased gentrification than being not proximate to a protected area, or even being a protected area. 

However, before we can conclude that these results are at all meaningful, we can test for spatial dependence using a Global Moran's L: 

```{r warning=FALSE}
bw <- poly2nb(b_sf, queen=T)
sacw<-nb2listw(bw, style="W", zero.policy = TRUE)

coo <- coordinates(as_Spatial(b_sf))
k1 <- knn2nb(knearneigh(coo))
critical.threshold <- max(unlist(nbdists(k1,coo)))

nb.dist.band <- dnearneigh(coo, 0, critical.threshold)
distances <- nbdists(nb.dist.band,coo)
invd1 <- lapply(distances, function(x) (1/x))
invd1a <- lapply(distances, function(x) (1/(x/100)))
invd.weights <- nb2listw(nb.dist.band,glist = invd1a,style = "B")

b_sf$gcode[is.na(b_sf$gcode)] <- 0
moran.mc(b_sf$gcode, invd.weights, nsim=200)
```

The presence of spatial autocorrelation means that the assumption of units being unaffected by nearby units in their outcome doesn't much hold here. I try to address this later using spatial weights, but methodologies for this are still developing, and this kind of work is largely beyond the scope of my project. 

### 3.2 Difference-in-difference

It's a bit contradictory to try this method of causal inference having just explained by why classic inference techniques aren't as helpful here, I still want to demonstrate how the data I generated can, with a bit more wrangling, be used alongside powerful econometric techniques. Here, I consider status as a ratio, which, while not strictly-speaking correct, is a stable enough assumption to not try to use an ordinal difference-in-difference approach. Most of the heavy lifting is done by the `did` package, and some results are shown below: 

```{r warning=FALSE, message=FALSE, results=FALSE}
load("~/Desktop/Code/Thesis/Data_for_Analysis/blong.Rdata")
library(did)
didtest <- did::att_gt(
  yname = "gstatus",
  tname = "Year",
  idname = "RAUMID",
  gname = "F_IN_KRAFT.x",
  xformla=~1,
  data = dfL_full
)

outsimp <- did::aggte(didtest, type="simple")

outdyn <- did::aggte(didtest, type="dynamic")
```
```{r echo=FALSE}
summary(didtest)
summary(outsimp)
summary(outdyn)

ggdid(outdyn)
```

Looking at the results, it seems like there is at least some effect from treatment on gentrification status increase shown by the average treatment effects on the treated (`ATT`) of `0.2892`, with a confidence interval not covering 0. However, the substantial amount of error shown by the plot reflects that there's quite a lot of variation in effects, and the results are further complicated by the presence of positive global spatial autocorrelation, which means we might have violated the stable unit value treatment assumption, a cornerstone of the difference-in-difference method! 

I'm still working on ironing this section out (I'm hoping to include an attempt at a survival analysis-style look into whether treatment effects the amount of time between each stage of gentrification, and a spatially weighted panel test) and more will definitely be added to this page as I make progress. 

## Conclusion 

I hope I've been able to show some of the work I've done these past few months -- as tiring and overwhelming as it's been at moments, it's also been an incredible opportunity to learn, and I'm so excited to keep pushing the project forward. 

As I said in the beginning, if you have any questions, concerns, or feedback, please don't hesitate to reach out! 
